# Config for pretrain_mlp_t_sudoku
# Based on commands file settings

defaults:
  - arch: trm
  - _self_

hydra:
  output_subdir: null

# Data path
data_paths: ['data/sudoku-extreme-50-aug-1000']
data_paths_test: []

evaluators: []

# Hyperparams - Training
global_batch_size: 768

epochs: 5000 # 50000
# eval is every eval_interval * (train dataset size) / global_batch_size batches
eval_interval: 500 # 5000
checkpoint_every_eval: False # True

eval_dataset_fraction: 0.1
eval_max_batches: 25

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 200 # 2000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 1.0
puzzle_emb_weight_decay: 1.0

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-4

seed: 0
min_eval_interval: 0 # when to start the eval

ema: True # use Exponential-Moving-Average
ema_rate: 0.999 # EMA-rate
freeze_weights: False # If True, freeze weights and only learn the embeddings

# Run name
run_name: sudoku_mlp_test

# Architecture overrides
arch:
  mlp_t: True
  pos_encodings: none
  L_layers: 2
  H_cycles: 3
  L_cycles: 6